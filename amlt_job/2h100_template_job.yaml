description: bitnet

target:
  service: aml
  name: alta2

# environment:
#   image: nvcr:v23.10
#   registry: shumingdocker.azurecr.io
#   setup:
#   - echo "master_addr:" "$$MASTER_ADDR"
#   - echo "master_port:" $$MASTER_PORT
#   - echo "node_rank:" $$OMPI_COMM_WORLD_RANK
#   username: shumingdocker

# target:
#   service: sing
#   name: msroctovc
#   resource_group: gcr-singularity-octo
#   # workspace_name: Workspace_NLC
#   workspace_name: NLC_Workspace

# target:
#   service: sing
#   name: msrresrchvc
#   resource_group: gcr-singularity-resrch
#   workspace_name: Workspace_NLC


# target:
#   service: sing
#   name: msroctobasicvc
#   resource_group: gcr-singularity-octo
#   workspace_name: Workspace_NLC


environment:
  image: buaahsh/nvcr:24.07



code:
  local_dir: $CONFIG_DIR/..

storage:
  lingjiejiang:
    storage_account_name: msranlpintern
    container_name: lingjiejiang
#   msranlp:
#     storage_account_name: msranlp
#     container_name: unilm
#   nlcredstone:
#     storage_account_name: nlcredstone
#     container_name: unilm
#   conversationhub:
#     storage_account_name: conversationhub
#     container_name: unilm
#   conversationhubhot:
#     storage_account_name: conversationhubhot
#     container_name: tengchaolv


search:
  job_template:
    name: PRJ-0349-A54-1_.58_-bit-LLMs-test-scaling
    sku: 2xG8
    identity: managed
    mpi: True
    process_count_per_node: 1
    command:
    - pip install -e ".[torch,metrics]"
    - pip install deepspeed==0.14.4
    - echo $${rank}
    - pip install -U transformers
    # - bash amlt_job/sas_mount.sh
    # - pip install -U flash-attn --no-build-isolation
    - pip install -U flash-attn==2.4.2 --no-build-isolation
    - export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    - ls /mnt/lingjiejiang
    # - FORCE_TORCHRUN=1 llamafactory-cli train bash_script/{rank}
    # Download and extract
    - wget https://aka.ms/downloadazcopy-v10-linux
    - tar -xvf downloadazcopy-v10-linux
    - ./azcopy_linux_amd64_*/azcopy copy 'https://msranlpintern.blob.core.windows.net/lingjiejiang/textual_aesthetics/model_checkpoint/bitnet-2B-v0/?sv=2023-01-03&st=2024-11-12T09%3A54%3A42Z&se=2024-11-19T09%3A54%3A00Z&skoid=4ca952ec-968b-41ca-95cb-e9b2a5fe0a02&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2024-11-12T09%3A54%3A42Z&ske=2024-11-19T09%3A54%3A00Z&sks=b&skv=2023-01-03&sr=c&sp=racwdxltf&sig=BEYQBkcT8J7uYRNCtvkqPSxYcUzIHhsyXeb%2B2w2xbvU%3D' 'model' --recursive
    - ls model
    - torchrun --nproc_per_node=8 --nnodes=2 --node_rank=$$OMPI_COMM_WORLD_RANK --master_addr="$$MASTER_ADDR" --master_port=$$MASTER_PORT src/train.py bash_script/{rank}
    submit_args:
      env:
        {"SINGULARITY_MPI_ENV":"-mca pml ucx --mca btl ^vader,tcp,openib -x NCCL_SOCKET_IFNAME=bond0 -x NCCL_IB_HCA=mlx5_0,mlx5_3,mlx5_4,mlx5_5,mlx5_6,mlx5_9,mlx5_10,mlx5_11 -x NCCL_DEBUG=INFO"}
      container_args:
        shm_size: 256g
    tags: [Project_Name:1.58-bit-LLMs, ProjectID:PRJ-0349-A54, Experiment:BitNet-scaling]

  type: grid
  max_trials: 500
  params:
    - name: rank
      spec: discrete
      # values: ['bitnet_glan1.6_2048_default_template_2e5_e2.yaml']
      # values: ['bitnet_glan2-1108_2048_default_template_2e5_e2.yaml']
      # values: ['bitnet_glan1.6_2048_default_template_1e5_e3.yaml']
      # values: ['bitnet_glan1.6_2048_default_template_3e5_e2.yaml']
      # values: ['bitnet_glan1.6_2048_default_template_2e5_e3.yaml']
      values: ['bitnet_glan1.5_2048_default_template_2e5_e2.yaml', 'bitnet_glan1.5_2048_default_template_2e5_e3.yaml', 'bitnet_glan1.5_2048_default_template_2e5_e5.yaml']

